services:
  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag-qdrant
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 5s
      timeout: 10s
      retries: 10
      start_period: 10s
    restart: unless-stopped
    networks:
      - rag-network

  # Ollama LLM Service (optional - use profile to enable)
  ollama:
    image: ollama/ollama:latest
    container_name: rag-ollama
    profiles:
      - ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - rag-network

  # RAG Application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-app
    ports:
      - "${APP_PORT:-7860}:7860"
    volumes:
      - ./data:/app/data
      - ./content:/app/content  # Mount content to ingest
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-python_knowledge}
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
      - GHCP_MODEL=${GHCP_MODEL:-gpt-4o}
      - GHCP_BASE_URL=${GHCP_BASE_URL:-https://api.githubcopilot.com}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      - GRADIO_SERVER_NAME=0.0.0.0
    depends_on:
      qdrant:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - rag-network
    # Use host network for Ollama access on macOS/Windows
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Schema Management Service (run on-demand)
  schema-manager:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-schema-manager
    volumes:
      - ./data:/app/data
      - ./content:/app/content
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
    depends_on:
      qdrant:
        condition: service_healthy
    networks:
      - rag-network
    profiles:
      - tools  # Only run when explicitly requested
    entrypoint: ["python", "schema_manager.py"]

  # Data Ingestion Service (run on-demand)
  ingest:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-ingest
    volumes:
      - ./data:/app/data
      - ./content:/app/content
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
    depends_on:
      qdrant:
        condition: service_healthy
    networks:
      - rag-network
    profiles:
      - tools  # Only run when explicitly requested
    entrypoint: ["python", "ingest_data.py"]

volumes:
  qdrant_storage:
    driver: local
  ollama_models:
    driver: local

networks:
  rag-network:
    driver: bridge
